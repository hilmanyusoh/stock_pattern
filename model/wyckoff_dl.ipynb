{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20db13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['127.0.0.1'], lbp = None)\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. https://docs.datastax.com/en/developer/python-driver/latest/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. https://docs.datastax.com/en/developer/python-driver/latest/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "               üöÄ ADVANCED DEEP LEARNING WYCKOFF ANALYSIS üöÄ\n",
      "================================================================================\n",
      "‚úÖ Successfully loaded 729 records for CPALL\n",
      "\n",
      "================================================================================\n",
      "STEP 1: Advanced Feature Engineering (35+ features)\n",
      "================================================================================\n",
      "‚úÖ Created 77 columns\n",
      "\n",
      "================================================================================\n",
      "STEP 2: Smart Target Labeling\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TARGET DISTRIBUTION (Multi-Strategy Voting)\n",
      "================================================================================\n",
      "Normal          :   290 ( 46.03%)\n",
      "Accumulation    :   210 ( 33.33%)\n",
      "Distribution    :   130 ( 20.63%)\n",
      "\n",
      "================================================================================\n",
      "STEP 3: Hybrid Model Training (LSTM+GRU)\n",
      "================================================================================\n",
      "\n",
      "üìä Original dataset: 600 samples\n",
      "üîÑ Applying data augmentation...\n",
      "üìà Augmented dataset: 2550 samples\n",
      "\n",
      "‚öñÔ∏è  Class Weights:\n",
      "   Class 0: 0.756\n",
      "   Class 1: 0.994\n",
      "   Class 2: 1.491\n",
      "\n",
      "üèóÔ∏è  Model Architecture:\n",
      "   Total Parameters: 230,499\n",
      "\n",
      "üöÄ Training Advanced Deep Learning Model...\n",
      "================================================================================\n",
      "Epoch 1/120\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.4423 - loss: 9.2650\n",
      "Epoch 1: val_accuracy improved from None to 0.46667, saving model to best_wyckoff_dl_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: finished saving model to best_wyckoff_dl_model.h5\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - accuracy: 0.4769 - loss: 8.9604 - val_accuracy: 0.4667 - val_loss: 8.3893 - learning_rate: 3.0000e-04\n",
      "Epoch 2/120\n",
      "\u001b[1m159/160\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.5233 - loss: 8.1963\n",
      "Epoch 2: val_accuracy improved from 0.46667 to 0.52222, saving model to best_wyckoff_dl_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: finished saving model to best_wyckoff_dl_model.h5\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 90ms/step - accuracy: 0.5431 - loss: 7.9608 - val_accuracy: 0.5222 - val_loss: 7.5394 - learning_rate: 3.0000e-04\n",
      "Epoch 3/120\n",
      "\u001b[1m122/160\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.5755 - loss: 7.3391"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from cassandra.cluster import Cluster\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (LSTM, Dense, Dropout, GRU, Bidirectional,\n",
    "                                     Input, Concatenate, BatchNormalization,\n",
    "                                     GlobalAveragePooling1D, Attention)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ========================================\n",
    "# 1. Configuration\n",
    "# ========================================\n",
    "CASSANDRA_HOST = '127.0.0.1'\n",
    "CASSANDRA_PORT = 9042\n",
    "SYMBOL = 'CPALL'\n",
    "WINDOW_SIZE = 30\n",
    "LEARNING_RATE = 0.0003\n",
    "EPOCHS = 120\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# ========================================\n",
    "# 2. Data Loading\n",
    "# ========================================\n",
    "def get_data(symbol):\n",
    "    try:\n",
    "        cluster = Cluster([CASSANDRA_HOST], port=CASSANDRA_PORT)\n",
    "        session = cluster.connect('data_stock')\n",
    "        query = f\"SELECT symbol, time, open, high, low, close, volume FROM candlestick_data WHERE symbol = '{symbol}' LIMIT 3000 ALLOW FILTERING\"\n",
    "        df = pd.DataFrame(list(session.execute(query)))\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df.sort_values('time', inplace=True)\n",
    "        cluster.shutdown()\n",
    "        print(f\"‚úÖ Successfully loaded {len(df)} records for {symbol}\")\n",
    "        return df.reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Connection Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ========================================\n",
    "# 3. Advanced Technical Indicators\n",
    "# ========================================\n",
    "def calculate_rsi(series, period=14):\n",
    "    \"\"\"RSI with improved calculation\"\"\"\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0).rolling(window=period, min_periods=1).mean()\n",
    "    loss = (-delta.clip(upper=0)).rolling(window=period, min_periods=1).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(series):\n",
    "    \"\"\"MACD with histogram\"\"\"\n",
    "    ema_12 = series.ewm(span=12).mean()\n",
    "    ema_26 = series.ewm(span=26).mean()\n",
    "    macd = ema_12 - ema_26\n",
    "    signal = macd.ewm(span=9).mean()\n",
    "    histogram = macd - signal\n",
    "    return macd, signal, histogram\n",
    "\n",
    "def calculate_bbands(series, period=20):\n",
    "    \"\"\"Bollinger Bands\"\"\"\n",
    "    sma = series.rolling(period).mean()\n",
    "    std = series.rolling(period).std()\n",
    "    upper = sma + (2 * std)\n",
    "    lower = sma - (2 * std)\n",
    "    bb_position = (series - lower) / (upper - lower + 1e-10)\n",
    "    bb_width = (upper - lower) / sma\n",
    "    return bb_position, bb_width\n",
    "\n",
    "def calculate_stochastic(df, period=14):\n",
    "    \"\"\"Stochastic Oscillator\"\"\"\n",
    "    low_min = df['low'].rolling(period).min()\n",
    "    high_max = df['high'].rolling(period).max()\n",
    "    k = 100 * (df['close'] - low_min) / (high_max - low_min + 1e-10)\n",
    "    d = k.rolling(3).mean()\n",
    "    return k, d\n",
    "\n",
    "def calculate_obv(df):\n",
    "    \"\"\"On-Balance Volume\"\"\"\n",
    "    obv = (np.sign(df['close'].diff()) * df['volume']).fillna(0).cumsum()\n",
    "    return obv\n",
    "\n",
    "def calculate_mfi(df, period=14):\n",
    "    \"\"\"Money Flow Index\"\"\"\n",
    "    typical_price = (df['high'] + df['low'] + df['close']) / 3\n",
    "    money_flow = typical_price * df['volume']\n",
    "    \n",
    "    positive_flow = pd.Series(0.0, index=df.index)\n",
    "    negative_flow = pd.Series(0.0, index=df.index)\n",
    "    \n",
    "    positive_flow[typical_price > typical_price.shift(1)] = money_flow[typical_price > typical_price.shift(1)]\n",
    "    negative_flow[typical_price < typical_price.shift(1)] = money_flow[typical_price < typical_price.shift(1)]\n",
    "    \n",
    "    mfr = positive_flow.rolling(period).sum() / (negative_flow.rolling(period).sum() + 1e-10)\n",
    "    mfi = 100 - (100 / (1 + mfr))\n",
    "    return mfi\n",
    "\n",
    "# ========================================\n",
    "# 4. Comprehensive Feature Engineering\n",
    "# ========================================\n",
    "def prepare_advanced_features(df):\n",
    "    \"\"\"Create 35+ advanced features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # === Basic Price Features ===\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    df['high_low_spread'] = (df['high'] - df['low']) / df['close']\n",
    "    df['close_open_spread'] = (df['close'] - df['open']) / df['open']\n",
    "    df['high_low_ratio'] = df['high'] / df['low']\n",
    "    \n",
    "    # === Volume Features ===\n",
    "    for period in [5, 10, 20, 30]:\n",
    "        df[f'volume_ma_{period}'] = df['volume'].rolling(period).mean()\n",
    "        df[f'volume_ratio_{period}'] = df['volume'] / (df[f'volume_ma_{period}'] + 1)\n",
    "    \n",
    "    df['volume_momentum'] = df['volume'].pct_change(5)\n",
    "    df['volume_std'] = df['volume'].rolling(20).std()\n",
    "    \n",
    "    # === Moving Averages ===\n",
    "    for period in [5, 10, 20, 50, 100]:\n",
    "        df[f'sma_{period}'] = df['close'].rolling(period).mean()\n",
    "        df[f'ema_{period}'] = df['close'].ewm(span=period).mean()\n",
    "    \n",
    "    # === Distance from MAs ===\n",
    "    for period in [10, 20, 50]:\n",
    "        df[f'dist_from_sma_{period}'] = (df['close'] - df[f'sma_{period}']) / df[f'sma_{period}']\n",
    "        df[f'dist_from_ema_{period}'] = (df['close'] - df[f'ema_{period}']) / df[f'ema_{period}']\n",
    "    \n",
    "    # === Technical Indicators ===\n",
    "    df['rsi'] = calculate_rsi(df['close'], 14)\n",
    "    df['rsi_slope'] = df['rsi'].diff(5)\n",
    "    \n",
    "    df['macd'], df['macd_signal'], df['macd_hist'] = calculate_macd(df['close'])\n",
    "    df['macd_cross'] = (df['macd'] > df['macd_signal']).astype(int)\n",
    "    \n",
    "    df['stoch_k'], df['stoch_d'] = calculate_stochastic(df)\n",
    "    df['bb_position'], df['bb_width'] = calculate_bbands(df['close'])\n",
    "    \n",
    "    df['obv'] = calculate_obv(df)\n",
    "    df['obv_slope'] = df['obv'].diff(5)\n",
    "    df['mfi'] = calculate_mfi(df, 14)\n",
    "    \n",
    "    # === Volatility ===\n",
    "    for period in [5, 10, 20]:\n",
    "        df[f'volatility_{period}'] = df['returns'].rolling(period).std()\n",
    "        df[f'atr_{period}'] = df['high_low_spread'].rolling(period).mean()\n",
    "    \n",
    "    # === Price Action ===\n",
    "    df['body'] = abs(df['close'] - df['open'])\n",
    "    df['upper_shadow'] = df['high'] - df[['close', 'open']].max(axis=1)\n",
    "    df['lower_shadow'] = df[['close', 'open']].min(axis=1) - df['low']\n",
    "    df['body_pct'] = df['body'] / (df['high_low_spread'] + 1e-10)\n",
    "    df['upper_shadow_pct'] = df['upper_shadow'] / (df['high_low_spread'] + 1e-10)\n",
    "    df['lower_shadow_pct'] = df['lower_shadow'] / (df['high_low_spread'] + 1e-10)\n",
    "    \n",
    "    # === Momentum ===\n",
    "    for period in [5, 10, 20]:\n",
    "        df[f'momentum_{period}'] = df['close'].diff(period)\n",
    "        df[f'roc_{period}'] = df['close'].pct_change(period)\n",
    "    \n",
    "    # === Trend Features ===\n",
    "    df['price_above_sma20'] = (df['close'] > df['sma_20']).astype(int)\n",
    "    df['price_above_sma50'] = (df['close'] > df['sma_50']).astype(int)\n",
    "    df['sma_trend'] = (df['sma_20'] > df['sma_50']).astype(int)\n",
    "    \n",
    "    # === Wyckoff Events ===\n",
    "    df['high_volume'] = (df['volume'] > df['volume_ma_30'] * 2.0).astype(int)\n",
    "    df['SC'] = ((df['high_volume'] == 1) & \n",
    "                (df['low'] == df['low'].rolling(30).min()) &\n",
    "                (df['close'] < df['open'])).astype(int)\n",
    "    df['BC'] = ((df['high_volume'] == 1) & \n",
    "                (df['high'] == df['high'].rolling(30).max()) &\n",
    "                (df['close'] < df['open'])).astype(int)\n",
    "    \n",
    "    # AR Detection\n",
    "    df['AR'] = 0\n",
    "    for i in df.index[df['SC'] | df['BC']]:\n",
    "        if i + 10 < len(df):\n",
    "            if df.loc[i, 'SC']:\n",
    "                ar_idx = df.loc[i:i+10, 'high'].idxmax()\n",
    "            else:\n",
    "                ar_idx = df.loc[i:i+10, 'low'].idxmin()\n",
    "            df.at[ar_idx, 'AR'] = 1\n",
    "    \n",
    "    # Spring Detection\n",
    "    support = df['low'].rolling(60).min().shift(1)\n",
    "    df['SPRING'] = ((df['low'] < support) & \n",
    "                    (df['close'] > support) & \n",
    "                    (df['high_volume'] == 1) &\n",
    "                    (df['close'] > df['open'])).astype(int)\n",
    "    \n",
    "    return df.dropna().reset_index(drop=True)\n",
    "\n",
    "# ========================================\n",
    "# 5. Smart Multi-Strategy Target Labeling\n",
    "# ========================================\n",
    "def create_smart_targets(df):\n",
    "    \"\"\"Multi-strategy voting system for balanced targets\"\"\"\n",
    "    df = df.copy()\n",
    "    df['target'] = 0\n",
    "    \n",
    "    # Thresholds\n",
    "    volume_high = df['volume_ratio_30'] > df['volume_ratio_30'].quantile(0.70)\n",
    "    \n",
    "    # Strategy 1: Price + Volume\n",
    "    price_low = df['close'] < df['sma_50']\n",
    "    price_high = df['close'] > df['sma_50']\n",
    "    \n",
    "    # Strategy 2: RSI\n",
    "    rsi_oversold = df['rsi'] < 40\n",
    "    rsi_overbought = df['rsi'] > 60\n",
    "    \n",
    "    # Strategy 3: MFI\n",
    "    mfi_oversold = df['mfi'] < 35\n",
    "    mfi_overbought = df['mfi'] > 65\n",
    "    \n",
    "    # Strategy 4: Bollinger Bands\n",
    "    bb_low = df['bb_position'] < 0.25\n",
    "    bb_high = df['bb_position'] > 0.75\n",
    "    \n",
    "    # Strategy 5: Stochastic\n",
    "    stoch_oversold = df['stoch_k'] < 20\n",
    "    stoch_overbought = df['stoch_k'] > 80\n",
    "    \n",
    "    # Strategy 6: MACD\n",
    "    macd_bullish = (df['macd'] > df['macd_signal']) & (df['macd_hist'] > 0)\n",
    "    macd_bearish = (df['macd'] < df['macd_signal']) & (df['macd_hist'] < 0)\n",
    "    \n",
    "    # Accumulation Voting\n",
    "    accum_votes = (\n",
    "        (price_low & volume_high).astype(int) +\n",
    "        rsi_oversold.astype(int) +\n",
    "        mfi_oversold.astype(int) +\n",
    "        bb_low.astype(int) +\n",
    "        stoch_oversold.astype(int) +\n",
    "        macd_bullish.astype(int) +\n",
    "        (df['close'] > df['open']).astype(int)\n",
    "    )\n",
    "    \n",
    "    # Distribution Voting\n",
    "    dist_votes = (\n",
    "        (price_high & volume_high).astype(int) +\n",
    "        rsi_overbought.astype(int) +\n",
    "        mfi_overbought.astype(int) +\n",
    "        bb_high.astype(int) +\n",
    "        stoch_overbought.astype(int) +\n",
    "        macd_bearish.astype(int) +\n",
    "        (df['close'] < df['open']).astype(int)\n",
    "    )\n",
    "    \n",
    "    # Assign labels (need 3+ votes)\n",
    "    df.loc[accum_votes >= 3, 'target'] = 1\n",
    "    df.loc[dist_votes >= 3, 'target'] = 2\n",
    "    \n",
    "    # Resolve conflicts\n",
    "    conflicts = (accum_votes >= 3) & (dist_votes >= 3)\n",
    "    df.loc[conflicts & (accum_votes > dist_votes), 'target'] = 1\n",
    "    df.loc[conflicts & (dist_votes > accum_votes), 'target'] = 2\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TARGET DISTRIBUTION (Multi-Strategy Voting)\")\n",
    "    print(\"=\"*80)\n",
    "    target_counts = df['target'].value_counts().sort_index()\n",
    "    for label, count in target_counts.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        label_name = ['Normal', 'Accumulation', 'Distribution'][label]\n",
    "        print(f\"{label_name:15} : {count:5} ({pct:6.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ========================================\n",
    "# 6. Data Augmentation\n",
    "# ========================================\n",
    "def augment_sequences(X, y, augment_factor=2):\n",
    "    \"\"\"Time series augmentation\"\"\"\n",
    "    X_aug, y_aug = [X], [y]\n",
    "    \n",
    "    for _ in range(augment_factor):\n",
    "        # Jittering\n",
    "        noise = np.random.normal(0, 0.01, X.shape)\n",
    "        X_jittered = X + noise\n",
    "        X_aug.append(X_jittered)\n",
    "        y_aug.append(y)\n",
    "        \n",
    "        # Scaling\n",
    "        scale = np.random.uniform(0.98, 1.02, (X.shape[0], 1, 1))\n",
    "        X_scaled = X * scale\n",
    "        X_aug.append(X_scaled)\n",
    "        y_aug.append(y)\n",
    "    \n",
    "    return np.concatenate(X_aug), np.concatenate(y_aug)\n",
    "\n",
    "# ========================================\n",
    "# 7. Advanced Hybrid Model\n",
    "# ========================================\n",
    "def create_hybrid_model(input_shape, num_classes=3):\n",
    "    \"\"\"Hybrid LSTM + GRU + Attention model\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Branch 1: Bidirectional LSTM\n",
    "    lstm_out = Bidirectional(LSTM(80, return_sequences=True))(inputs)\n",
    "    lstm_out = BatchNormalization()(lstm_out)\n",
    "    lstm_out = Dropout(0.35)(lstm_out)\n",
    "    \n",
    "    # Branch 2: Bidirectional GRU\n",
    "    gru_out = Bidirectional(GRU(80, return_sequences=True))(inputs)\n",
    "    gru_out = BatchNormalization()(gru_out)\n",
    "    gru_out = Dropout(0.35)(gru_out)\n",
    "    \n",
    "    # Merge branches\n",
    "    merged = Concatenate()([lstm_out, gru_out])\n",
    "    \n",
    "    # Additional LSTM layer\n",
    "    x = LSTM(64, return_sequences=True)(merged)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Global Average Pooling\n",
    "    pooled = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    dense = Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(pooled)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dropout(0.4)(dense)\n",
    "    \n",
    "    dense = Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(dense)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dropout(0.3)(dense)\n",
    "    \n",
    "    outputs = Dense(num_classes, activation='softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# ========================================\n",
    "# 8. Advanced Training\n",
    "# ========================================\n",
    "def train_advanced_dl_model(df, window_size=WINDOW_SIZE):\n",
    "    \"\"\"Train with advanced techniques\"\"\"\n",
    "    \n",
    "    # Select features\n",
    "    feature_cols = [\n",
    "        'returns', 'log_returns', 'volume_ratio_10', 'volume_ratio_30',\n",
    "        'rsi', 'rsi_slope', 'macd', 'macd_hist',\n",
    "        'stoch_k', 'stoch_d', 'mfi', 'obv_slope',\n",
    "        'dist_from_sma_10', 'dist_from_sma_20', 'dist_from_sma_50',\n",
    "        'dist_from_ema_10', 'dist_from_ema_20',\n",
    "        'volatility_5', 'volatility_20', 'atr_10',\n",
    "        'bb_position', 'bb_width',\n",
    "        'body_pct', 'upper_shadow_pct', 'lower_shadow_pct',\n",
    "        'momentum_5', 'momentum_20',\n",
    "        'sma_trend', 'price_above_sma20'\n",
    "    ]\n",
    "    \n",
    "    # RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    scaled_data = scaler.fit_transform(df[feature_cols])\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(scaled_data)):\n",
    "        X.append(scaled_data[i-window_size:i])\n",
    "        y.append(df['target'].iloc[i])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"\\nüìä Original dataset: {len(X)} samples\")\n",
    "    \n",
    "    # Split\n",
    "    split = int(len(X) * 0.85)\n",
    "    X_train, X_val = X[:split], X[split:]\n",
    "    y_train, y_val = y[:split], y[split:]\n",
    "    \n",
    "    # Data Augmentation\n",
    "    print(\"üîÑ Applying data augmentation...\")\n",
    "    X_train_aug, y_train_aug = augment_sequences(X_train, y_train, augment_factor=2)\n",
    "    print(f\"üìà Augmented dataset: {len(X_train_aug)} samples\")\n",
    "    \n",
    "    # Class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train_aug),\n",
    "        y=y_train_aug\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    print(\"\\n‚öñÔ∏è  Class Weights:\")\n",
    "    for cls, weight in class_weight_dict.items():\n",
    "        print(f\"   Class {cls}: {weight:.3f}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_hybrid_model((window_size, len(feature_cols)))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = AdamW(learning_rate=LEARNING_RATE, weight_decay=0.001)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüèóÔ∏è  Model Architecture:\")\n",
    "    print(f\"   Total Parameters: {model.count_params():,}\")\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=8,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'best_wyckoff_dl_model.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Training\n",
    "    print(\"\\nüöÄ Training Advanced Deep Learning Model...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_aug, y_train_aug,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    df['dl_zone'] = 0\n",
    "    full_preds = np.argmax(model.predict(X, verbose=0), axis=1)\n",
    "    df.iloc[window_size:, df.columns.get_loc('dl_zone')] = full_preds\n",
    "    \n",
    "    return df, model, history, (X_val, y_val), scaler\n",
    "\n",
    "# ========================================\n",
    "# 9. Visualization\n",
    "# ========================================\n",
    "def plot_dashboard(df):\n",
    "    \"\"\"Enhanced dashboard\"\"\"\n",
    "    plt.rcParams['font.family'] = 'Tahoma'\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(18, 9))\n",
    "    \n",
    "    zone_types = {\n",
    "        1: ('#FFD700', 'Accumulation (‡∏™‡∏∞‡∏™‡∏°)'),\n",
    "        2: ('#FF6347', 'Distribution (‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢)')\n",
    "    }\n",
    "    \n",
    "    for zone_id, (color, label_name) in zone_types.items():\n",
    "        subset = df[df['dl_zone'] == zone_id].copy()\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            start_t = subset['time'].iloc[0]\n",
    "            \n",
    "            for i in range(1, len(subset)):\n",
    "                time_gap = (subset['time'].iloc[i] - subset['time'].iloc[i-1]).days\n",
    "                is_last = (i == len(subset) - 1)\n",
    "                \n",
    "                if time_gap > 5 or is_last:\n",
    "                    end_t = subset['time'].iloc[i] if is_last else subset['time'].iloc[i-1]\n",
    "                    box_data = df[(df['time'] >= start_t) & (df['time'] <= end_t)]\n",
    "                    \n",
    "                    if len(box_data) > 0:\n",
    "                        rect_start = mdates.date2num(start_t)\n",
    "                        rect_width = mdates.date2num(end_t) - rect_start\n",
    "                        y_min = box_data['low'].min() * 0.99\n",
    "                        y_max = box_data['high'].max() * 1.01\n",
    "                        \n",
    "                        rect = patches.Rectangle(\n",
    "                            (rect_start, y_min), rect_width, y_max - y_min,\n",
    "                            linewidth=0, facecolor=color, alpha=0.2,\n",
    "                            label=label_name, zorder=1\n",
    "                        )\n",
    "                        ax.add_patch(rect)\n",
    "                    \n",
    "                    start_t = subset['time'].iloc[i]\n",
    "    \n",
    "    ax.plot(df['time'], df['close'], 'k-', linewidth=1.5, label='Close', zorder=3, alpha=0.8)\n",
    "    ax.plot(df['time'], df['sma_20'], 'b--', linewidth=1, label='SMA 20', alpha=0.6)\n",
    "    ax.plot(df['time'], df['sma_50'], color='orange', linewidth=1, label='SMA 50', alpha=0.6)\n",
    "    \n",
    "    for event, marker, color, size, label in [\n",
    "        ('SC', 'v', 'red', 120, 'SC'),\n",
    "        ('AR', 'D', 'blue', 80, 'AR'),\n",
    "        ('SPRING', '^', 'green', 150, 'Spring')\n",
    "    ]:\n",
    "        points = df[df[event] == 1]\n",
    "        if len(points) > 0:\n",
    "            y_val = points['low'] if event in ['SC', 'SPRING'] else points['high']\n",
    "            ax.scatter(points['time'], y_val, color=color, marker=marker, s=size,\n",
    "                      edgecolor='black' if event != 'AR' else 'white',\n",
    "                      linewidth=1.5, label=label, zorder=10)\n",
    "    \n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    ax.set_title(f'Advanced Deep Learning Wyckoff Analysis: {SYMBOL}', \n",
    "                fontsize=18, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Date', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Price (THB)', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), loc='upper left', \n",
    "             fontsize=10, frameon=True, shadow=True)\n",
    "    \n",
    "    ax.grid(True, linestyle=':', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "    axes[0].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "    axes[0].set_title('Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "    axes[1].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "    axes[1].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ========================================\n",
    "# 10. Main Execution\n",
    "# ========================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" \" * 15 + \"üöÄ ADVANCED DEEP LEARNING WYCKOFF ANALYSIS üöÄ\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df_raw = get_data(SYMBOL)\n",
    "    \n",
    "    if df_raw is not None:\n",
    "        # Feature Engineering\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 1: Advanced Feature Engineering (35+ features)\")\n",
    "        print(\"=\"*80)\n",
    "        df_feat = prepare_advanced_features(df_raw)\n",
    "        print(f\"‚úÖ Created {len(df_feat.columns)} columns\")\n",
    "        \n",
    "        # Target Labeling\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 2: Smart Target Labeling\")\n",
    "        print(\"=\"*80)\n",
    "        df_labeled = create_smart_targets(df_feat)\n",
    "        \n",
    "        # Train Model\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 3: Hybrid Model Training (LSTM+GRU)\")\n",
    "        print(\"=\"*80)\n",
    "        df_final, model, history, (X_val, y_val), scaler = train_advanced_dl_model(df_labeled)\n",
    "        \n",
    "        # Training History\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 4: Training Visualization\")\n",
    "        print(\"=\"*80)\n",
    "        plot_training_history(history)\n",
    "        \n",
    "        # Dashboard\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 5: Dashboard Visualization\")\n",
    "        print(\"=\"*80)\n",
    "        plot_dashboard(df_final)\n",
    "        \n",
    "        # Evaluation\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 6: Model Evaluation\")\n",
    "        print(\"=\"*80)\n",
    "        y_pred = np.argmax(model.predict(X_val, verbose=0), axis=1)\n",
    "        \n",
    "        print(\"\\nüìã Classification Report:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(classification_report(y_val, y_pred,\n",
    "                                   target_names=['Normal', 'Accumulation', 'Distribution'],\n",
    "                                   digits=4))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn_r',\n",
    "                   xticklabels=['Normal', 'Accum', 'Dist'],\n",
    "                   yticklabels=['Normal', 'Accum', 'Dist'])\n",
    "        plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('True')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary\n",
    "        zone_dist = df_final['dl_zone'].value_counts().sort_index()\n",
    "        zone_names = {0: 'Normal', 1: 'Accumulation', 2: 'Distribution'}\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üéØ PREDICTED ZONE DISTRIBUTION\")\n",
    "        print(\"=\"*80)\n",
    "        for zone_id, count in zone_dist.items():\n",
    "            pct = (count / len(df_final)) * 100\n",
    "            print(f\"{zone_names[zone_id]:15} : {count:5} bars ({pct:6.2f}%)\")\n",
    "        \n",
    "        # Final Metrics\n",
    "        best_val_loss = min(history.history['val_loss'])\n",
    "        best_val_acc = max(history.history['val_accuracy'])\n",
    "        best_train_acc = max(history.history['accuracy'])\n",
    "        f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üèÜ FINAL RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Best Validation Loss     : {best_val_loss:.4f}\")\n",
    "        print(f\"Best Validation Accuracy : {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "        print(f\"Best Training Accuracy   : {best_train_acc:.4f} ({best_train_acc*100:.2f}%)\")\n",
    "        print(f\"F1 Score (Weighted)      : {f1:.4f}\")\n",
    "        print(f\"Overfitting Gap          : {(best_train_acc - best_val_acc)*100:.2f}%\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"‚úÖ Process completed successfully!\")\n",
    "        print(\"üíæ Best model saved as: best_wyckoff_dl_model.h5\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå Failed to load data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
